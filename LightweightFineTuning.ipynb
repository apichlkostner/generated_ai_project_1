{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4322b",
   "metadata": {},
   "source": [
    "### Load the datasdet dair-air/emotion and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506ec15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "i feel a divine calling to become an lsd chemist the pub shroomery message board link href smarty templates css www : 1\n",
      "i keep in the fridge and take out whenever i feel too hot spray on my face and it cools me down immediately let alone it smells like oranges : 2\n",
      "i feel sure a new necklace will come from this afternoon of beach combing : 1\n",
      "i feel like i want to make something but the house is so messy and i am still finishing up christmas gift knitting : 0\n",
      "i was feeling determined it didnt take long for me to start nomming on naughty stuff again : 1\n",
      "i had to be transgender and this very brainwashing attempt is now making me feel so horrible as though im trying to deceive physicians here in germany into believing that i am intersex : 0\n",
      "im taking this to heart and feel that the gentle age of is the perfect time and place to engage in some fearlessness and wholesale change : 2\n",
      "i still second guess myself and still have a terrible time making definitive decisions but there are certain truths that i do know about myself and i feel assured by those truths : 1\n",
      "i present two photos of myself side by side and in one photo i remember feeling cute that day : 1\n",
      "i spent the following months in a drug induced haze incapable of thought or feeling but it wasn t anything as glamorous : 1\n",
      "\n",
      "Labels: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# print some random featues and the labels\n",
    "print(\"Features:\")\n",
    "indices = random.sample(range(len(ds[\"train\"])), 10)\n",
    "for i in indices:\n",
    "    print(\"{} : {}\".format(ds[\"train\"]['text'][i], ds[\"train\"]['label'][i]))\n",
    "\n",
    "print(\"\\nLabels: {}\".format(ds[\"train\"].features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "# create data structures for further processing\n",
    "\n",
    "# names of the splits\n",
    "splits=list(ds.keys())\n",
    "# number of classes\n",
    "num_classes=len(ds[\"train\"].features[\"label\"].names)\n",
    "\n",
    "# Dictionairies to translate between label string and label number\n",
    "id2label = dict(zip(range(num_classes), ds['train'].features['label'].names))\n",
    "label2id = dict(zip(ds['train'].features['label'].names, range(num_classes)))\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: [PAD]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Use GPT-2 as a small base model\n",
    "# Create a variant with classification head\n",
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add tokens to the dataset\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add the padding token which is missing in GPT-2\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(\"Padding token: {}\".format(tokenizer.pad_token))\n",
    "\n",
    "# metric function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 01:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.199998</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.142866</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.224700</td>\n",
       "      <td>1.121378</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.224700</td>\n",
       "      <td>1.089682</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=1.1781553268432616, metrics={'train_runtime': 71.2181, 'train_samples_per_second': 1123.31, 'train_steps_per_second': 11.233, 'total_flos': 2309089289011200.0, 'train_loss': 1.1781553268432616, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "temp_path = \"/tmp\"\n",
    "save_path = \"./data\"\n",
    "\n",
    "model_name = \"gpt2_classification\"\n",
    "checkpoint_dir = os.path.join(temp_path, model_name)\n",
    "save_dir_base = os.path.join(save_path, model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0896823406219482, 'eval_accuracy': 0.585, 'eval_runtime': 1.3099, 'eval_samples_per_second': 1526.873, 'eval_steps_per_second': 15.269, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "original_performance=trainer.evaluate()\n",
    "print(original_performance)\n",
    "\n",
    "model.save_pretrained(save_dir_base, save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 594,432 || all params: 125,039,616 || trainable%: 0.4754\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use Lora for PEFT\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "# adding PEFT modifies the base model in-place\n",
    "# so it should be saved for restoring the PEFT model later\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "model_name = \"gpt2_classification_lora\"\n",
    "checkpoint_dir = os.path.join(temp_path, model_name)\n",
    "save_dir = os.path.join(save_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 02:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239889</td>\n",
       "      <td>0.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152287</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.136047</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.125707</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.121623</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ssd1/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/media/ssd1/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/media/ssd1/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/media/ssd1/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/media/ssd1/venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.20044589519500733, metrics={'train_runtime': 142.8262, 'train_samples_per_second': 560.121, 'train_steps_per_second': 5.601, 'total_flos': 2325225977856000.0, 'train_loss': 0.20044589519500733, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "model_lora.save_pretrained(save_dir, save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModelForTokenClassification\n",
    "\n",
    "# loading the model\n",
    "model_base = AutoModelForSequenceClassification.from_pretrained(save_dir_base)\n",
    "model_loaded = PeftModelForTokenClassification.from_pretrained(model_base, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_evaluate = Trainer(\n",
    "    model=model_loaded,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis_lora_evaluate\",\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "fine_tuned_performance=trainer_evaluate.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:   {'eval_loss': 1.0896823406219482, 'eval_accuracy': 0.585, 'eval_runtime': 1.3099, 'eval_samples_per_second': 1526.873, 'eval_steps_per_second': 15.269, 'epoch': 5.0}\n",
      "Fine-Tuned Model: {'eval_loss': 0.121622733771801, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.933, 'eval_runtime': 1.406, 'eval_samples_per_second': 1422.447, 'eval_steps_per_second': 14.224}\n",
      "Original Model accurcy:    0.585\n",
      "Fine-Tuned Model accurcy:  0.933\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Original Model:  \", original_performance)\n",
    "print(\"Fine-Tuned Model:\", fine_tuned_performance)\n",
    "\n",
    "print(\"Original Model accurcy:   \", original_performance['eval_accuracy'])\n",
    "print(\"Fine-Tuned Model accurcy: \", fine_tuned_performance['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c5c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
