{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4322b",
   "metadata": {},
   "source": [
    "### Load the datasdet dair-air/emotion and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506ec15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "i feel more and more dissatisfied with each passing weekend : 3\n",
      "i feel slightly naughty holding this cd seeing as it doesnt officially release until tuesday : 2\n",
      "i was feeling bouncy so i added a few of my go to tangles around it i rather like the spiraling effect achieved : 1\n",
      "i feel like he should have waited for a girl who was less messy : 0\n",
      "i can look back likely years from now realize the impact of several lessons learned through the course of a season that just had that feel of something special and know that even if nothing in my tenure comes close to this again i will always have : 1\n",
      "i still feel a little bit listless but im coping with it by getting as much work done as possible to distract myself and trying not to overthink anything : 0\n",
      "i spent a lot of time earlier this year feeling stressed out about capacity and resistant to stretching it because it felt like stretching me : 0\n",
      "i hurt their feelings for refusing to listen to their spiteful hurtful sniping at others : 3\n",
      "i am still trying to find my footing and after three years in i feel just as shaky as ever : 4\n",
      "i did not feel dangerous enough to get in : 3\n",
      "\n",
      "Labels: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# print some random featues and the labels\n",
    "print(\"Features:\")\n",
    "indices = random.sample(range(len(ds[\"train\"])), 10)\n",
    "for i in indices:\n",
    "    print(\"{} : {}\".format(ds[\"train\"]['text'][i], ds[\"train\"]['label'][i]))\n",
    "\n",
    "print(\"\\nLabels: {}\".format(ds[\"train\"].features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "# create data structures for further processing\n",
    "\n",
    "# names of the splits\n",
    "splits=list(ds.keys())\n",
    "# number of classes\n",
    "num_classes=len(ds[\"train\"].features[\"label\"].names)\n",
    "\n",
    "# Dictionairies to translate between label string and label number\n",
    "id2label = dict(zip(range(num_classes), ds['train'].features['label'].names))\n",
    "label2id = dict(zip(ds['train'].features['label'].names, range(num_classes)))\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d125f7",
   "metadata": {},
   "source": [
    "Create a base model with added padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd66c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "\n",
    "# Create a base model variant with classification head\n",
    "def create_base_model(model_id):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id, \n",
    "        num_labels=num_classes,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        device_map=device)\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Use GPT-2 as a small base model\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = create_base_model(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add tokens to the dataset\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add the padding token which is missing in GPT-2\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# metric function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 00:58, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.298004</td>\n",
       "      <td>0.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.242040</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.225004</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.314000</td>\n",
       "      <td>1.219356</td>\n",
       "      <td>0.543000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=640, training_loss=1.2838828802108764, metrics={'train_runtime': 58.6693, 'train_samples_per_second': 1090.86, 'train_steps_per_second': 10.909, 'total_flos': 1848843351244800.0, 'train_loss': 1.2838828802108764, 'epoch': 4.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "temp_path = \"/tmp\"\n",
    "save_path = \"./data\"\n",
    "\n",
    "model_name = \"gpt2_classification\"\n",
    "checkpoint_dir = os.path.join(temp_path, model_name)\n",
    "save_dir_base = os.path.join(save_path, model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2193557024002075, 'eval_accuracy': 0.543, 'eval_runtime': 1.3184, 'eval_samples_per_second': 1517.005, 'eval_steps_per_second': 15.17, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "original_performance=trainer.evaluate()\n",
    "print(original_performance)\n",
    "\n",
    "model.save_pretrained(save_dir_base, save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 594,432 || all params: 125,038,848 || trainable%: 0.4754\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use Lora for PEFT\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "# adding PEFT modifies the base model in-place\n",
    "# so it should be saved for restoring the PEFT model later\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "model_name = \"gpt2_classification_lora\"\n",
    "checkpoint_dir = os.path.join(temp_path, model_name)\n",
    "save_dir = os.path.join(save_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 01:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.126496</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.122841</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=640, training_loss=0.22506903409957885, metrics={'train_runtime': 113.8406, 'train_samples_per_second': 562.19, 'train_steps_per_second': 5.622, 'total_flos': 1861763687424000.0, 'train_loss': 0.22506903409957885, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "model_lora.save_pretrained(save_dir, save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModelForTokenClassification\n",
    "\n",
    "# loading the model\n",
    "#model_base = AutoModelForSequenceClassification.from_pretrained(save_dir_base)\n",
    "# or build base model again\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model_base = create_base_model(model_id)\n",
    "\n",
    "model_loaded = PeftModelForTokenClassification.from_pretrained(model_base, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_evaluate = Trainer(\n",
    "    model=model_loaded,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis_lora_evaluate\",\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "fine_tuned_performance=trainer_evaluate.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:   {'eval_loss': 1.2193557024002075, 'eval_accuracy': 0.543, 'eval_runtime': 1.3184, 'eval_samples_per_second': 1517.005, 'eval_steps_per_second': 15.17, 'epoch': 4.0}\n",
      "Fine-Tuned Model: {'eval_loss': 0.12284120172262192, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.9285, 'eval_runtime': 1.3973, 'eval_samples_per_second': 1431.381, 'eval_steps_per_second': 14.314}\n",
      "Original Model accurcy:    0.543\n",
      "Fine-Tuned Model accurcy:  0.9285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Original Model:  \", original_performance)\n",
    "print(\"Fine-Tuned Model:\", fine_tuned_performance)\n",
    "\n",
    "print(\"Original Model accurcy:   \", original_performance['eval_accuracy'])\n",
    "print(\"Fine-Tuned Model accurcy: \", fine_tuned_performance['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e46c59",
   "metadata": {},
   "source": [
    "### Use different Quantization: QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 594,432 || all params: 125,038,848 || trainable%: 0.4754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ssd1/venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/800 00:39 < 00:59, 8.08 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248460</td>\n",
       "      <td>0.910500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/20 00:00 < 00:00, 18.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "temp_path = \"/tmp\"\n",
    "save_path = \"./data\"\n",
    "\n",
    "model_name = \"gpt2_classification_4bit_lora\"\n",
    "checkpoint_dir = os.path.join(temp_path, model_name)\n",
    "save_dir_base = os.path.join(save_path, model_name)\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model4b = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=\"auto\")\n",
    "\n",
    "model4b.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "for param in model4b.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# peft model\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "model4bl = get_peft_model(model4b, peft_config)\n",
    "model4bl.print_trainable_parameters()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model4bl,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=100,\n",
    "        per_device_eval_batch_size=100,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "validation_lora_q4 = trainer.evaluate()\n",
    "model4bl.save_pretrained(save_dir, save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c674ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Model accuracy:         \", original_performance['eval_accuracy'])\n",
    "print(\"Fine-Tuned Model accuracy:       \", fine_tuned_performance['eval_accuracy'])\n",
    "print(\"Fine-Tuned Model 4 bit accuracy: \", validation_lora_q4['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88eb0c",
   "metadata": {},
   "source": [
    "### Experiment with different LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74db434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModelForTokenClassification, LoraConfig, TaskType, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def create_lora_config(r, lora_alpha, lora_dropout):\n",
    "    peft_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.TOKEN_CLS,\n",
    "        fan_in_fan_out=True,\n",
    "    )\n",
    "\n",
    "    return peft_config\n",
    "\n",
    "def create_lora_model(peft_config):\n",
    "    model_base = create_base_model(model_id)\n",
    "    model_lora = get_peft_model(model_base, peft_config)\n",
    "\n",
    "    return model_lora\n",
    "    \n",
    "def create_trainer(model, learning_rate, weight_decay):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir='/tmp',\n",
    "            per_device_train_batch_size=50,\n",
    "            per_device_eval_batch_size=50,\n",
    "            num_train_epochs=4,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"test\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def evaluate_model(model):\n",
    "    eval = model.evaluate()\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e53326",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in [8, 4, 2]:\n",
    "    dropout = 0.1\n",
    "    learning_rate = 2e-3\n",
    "    weight_decay = 0.01\n",
    "    alpha = 2 * r\n",
    "    config = create_lora_config(r, alpha, dropout)\n",
    "    model = create_lora_model(config)\n",
    "    trainer = create_trainer(model, learning_rate, weight_decay)\n",
    "\n",
    "    print(\"Start training a model with R={}, alpha={} droptout={}\".format(r, alpha, dropout))\n",
    "\n",
    "    trainer.train()\n",
    "    eval = trainer.evaluate()\n",
    "\n",
    "    accuracy = eval['eval_accuracy']\n",
    "    results.append({'r': r, 'alpha': alpha, 'dropout': dropout, 'learning_rate':learning_rate, 'weight_decay': weight_decay, 'accuracy': accuracy})\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in [2e-4, 2e-3, 2e-2]:\n",
    "    dropout = 0.1\n",
    "    r = 8\n",
    "    weight_decay = 0.01\n",
    "    alpha = 2 * r\n",
    "    config = create_lora_config(r, alpha, dropout)\n",
    "    model = create_lora_model(config)\n",
    "    trainer = create_trainer(model, learning_rate, weight_decay)\n",
    "\n",
    "    print(\"Start training a model with R={}, alpha={} droptout={}\".format(r, alpha, dropout))\n",
    "\n",
    "    trainer.train()\n",
    "    eval = trainer.evaluate()\n",
    "\n",
    "    accuracy = eval['eval_accuracy']\n",
    "    results.append({'r': r, 'alpha': alpha, 'dropout': dropout, 'learning_rate':learning_rate, 'weight_decay': weight_decay, 'accuracy': accuracy})\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results)model_id = \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b515cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dropout in [0.01, 0.1, 0.5]:\n",
    "    learning_rate = 2e-3\n",
    "    r = 8\n",
    "    weight_decay = 0.01\n",
    "    alpha = 2 * r\n",
    "    config = create_lora_config(r, alpha, dropout)\n",
    "    model = create_lora_model(config)\n",
    "    trainer = create_trainer(model, learning_rate, weight_decay)\n",
    "\n",
    "    print(\"Start training a model with R={}, alpha={} droptout={}\".format(r, alpha, dropout))\n",
    "\n",
    "    trainer.train()\n",
    "    eval = trainer.evaluate()\n",
    "\n",
    "    accuracy = eval['eval_accuracy']\n",
    "    results.append({'r': r, 'alpha': alpha, 'dropout': dropout, 'learning_rate':learning_rate, 'weight_decay': weight_decay, 'accuracy': accuracy})\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_decay in [0.001, 0.01, 0.1]:\n",
    "    learning_rate = 2e-3\n",
    "    dropout = 0.1\n",
    "    r = 8\n",
    "    alpha = 2 * r\n",
    "    config = create_lora_config(r, alpha, dropout)\n",
    "    model = create_lora_model(config)\n",
    "    trainer = create_trainer(model, learning_rate, weight_decay)\n",
    "\n",
    "    print(\"Start training a model with R={}, alpha={} droptout={}\".format(r, alpha, dropout))\n",
    "\n",
    "    trainer.train()\n",
    "    eval = trainer.evaluate()\n",
    "\n",
    "    accuracy = eval['eval_accuracy']\n",
    "    results.append({'r': r, 'alpha': alpha, 'dropout': dropout, 'learning_rate':learning_rate, 'weight_decay': weight_decay, 'accuracy': accuracy})\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fc9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
